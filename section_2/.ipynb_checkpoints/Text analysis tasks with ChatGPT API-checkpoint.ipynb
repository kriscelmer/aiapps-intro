{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eb6c555",
   "metadata": {},
   "source": [
    "# Text analysis tasks with ChatGPT API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ba4bb0",
   "metadata": {},
   "source": [
    "## Text analysis tasks:\n",
    "\n",
    "- Text summarization\n",
    "- Extraction of topics, named entities, etc.\n",
    "- Sentiment analysis\n",
    "- Translation to other languages\n",
    "- Rephrasing to correct or address a need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c94dae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b0d42b",
   "metadata": {},
   "source": [
    "### *Imports and declarations*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import wikipedia\n",
    "import tiktoken\n",
    "from langchain import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "llm_model = OpenAI(temperature=0.0)\n",
    "\n",
    "tokenizer = tiktoken.encoding_for_model(llm_model.model_name)\n",
    "\n",
    "# Cost of executing ChatGPT calls is accumulated in 'total_cost'\n",
    "# Summary is printed at the end of this notebook\n",
    "total_cost = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e3a24e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e067c",
   "metadata": {},
   "source": [
    "### Summarize Wikipedia article on GPT-3 \n",
    "\n",
    "Python Wikipedia library documentation: https://wikipedia.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42acf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, length, llm=llm_model, print_full_prompt=False):\n",
    "    # text and length must be valid strings, length should be a string representation of an integer\n",
    "    global total_cost\n",
    "    \n",
    "    summarization_template_string = \"\"\"\n",
    "    Summarize the text delimited by tripple backticks in {length} words.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    summarization_prompt_template = PromptTemplate(\n",
    "        input_variables=[\"text\", \"length\"],\n",
    "        template=summarization_template_string\n",
    "    )\n",
    "    \n",
    "    model_input = summarization_prompt_template.format(text=text, length=length)\n",
    "\n",
    "    if print_full_prompt:\n",
    "        print(f\"Full prompt:\\n{model_input}\\n\")\n",
    "    \n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e8a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia page on GPT-3: https://en.wikipedia.org/wiki/GPT-3\n",
    "\n",
    "wikipedia.set_lang(\"en\")\n",
    "gpt3_article = wikipedia.page(\"GPT-3\", auto_suggest=False).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e80fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gpt3_article[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d333c",
   "metadata": {},
   "source": [
    "Check the article lenght in tokens to assure it fits into LLM's input limitation (together with prompt template text), which is 4096 tokens for GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2418b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.encode(gpt3_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae64ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_summary = summarize(gpt3_article, length=\"200\", print_full_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186d29b-1655-4f4f-9878-eae6fe4261a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Summary:\\n{gpt3_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words in the summary\n",
    "import re\n",
    "\n",
    "len(re.findall(r'\\w+', gpt3_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count tokens in the summary\n",
    "\n",
    "len(tokenizer.encode(gpt3_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bab62e8",
   "metadata": {},
   "source": [
    "### Summarize Wikipedia article on GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d6b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia page on GPT-4: https://en.wikipedia.org/wiki/GPT-4\n",
    "\n",
    "gpt4_article = wikipedia.page(\"GPT-4\", auto_suggest=False).content\n",
    "len(tokenizer.encode(gpt4_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8b2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_summary = summarize(gpt4_article, length=\"200\")\n",
    "    \n",
    "print(f\"Summary:\\n{gpt4_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc23d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_gpt4_summary = summarize(gpt4_article, length=\"100\")\n",
    "    \n",
    "print(f\"Summary:\\n{short_gpt4_summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f65fef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count words in the short summary\n",
    "\n",
    "len(re.findall(r'\\w+', short_gpt4_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05162eb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69b334",
   "metadata": {},
   "source": [
    "## Extract topics, named entities, etc. from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text, topic, llm=llm_model):\n",
    "    # text and topics must be valid strings\n",
    "    global total_cost\n",
    "    \n",
    "    extraction_template_string = \"\"\"\n",
    "    Extract {topic} from the text delimited by tripple backticks.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    extraction_prompt_template = PromptTemplate.from_template(extraction_template_string)\n",
    "    \n",
    "    model_input = extraction_prompt_template.format(text=text, topic=topic)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e87aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract(gpt3_summary, \"main topic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract(gpt4_summary, \"main topic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee449dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract(gpt3_summary, \"list of model names\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract(gpt4_summary, \"list of applications\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a24598",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0648b20",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6288708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(text, llm=llm_model):\n",
    "    global total_cost\n",
    "    \n",
    "    sentiment_template_string = \"\"\"\n",
    "    Classify the sentiment expressed in the review delimited by tripple backticks.\\\n",
    "    review: ```{text}```\n",
    "    \"\"\"\n",
    "    sentiment_prompt_template = PromptTemplate.from_template(sentiment_template_string)\n",
    "\n",
    "    model_input = sentiment_prompt_template.format(text=text)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = \"\"\"\n",
    "I purchased the PixelPioneer Quantum 60\" and it's a game-changer.\n",
    "The 4K resolution is stunning and the smart features are easy to use.\n",
    "Worth every penny! - George, Liverpool\"\"\"\n",
    "\n",
    "print(sentiment_analysis(review_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42abc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_2 = \"\"\"\n",
    "I'm not happy with the VisionCast UltraView 43\".\n",
    "The picture quality is subpar and the TV arrived with a scratch on the screen.\n",
    "I expected better quality control. - Sarah, Los Angeles\"\"\"\n",
    "\n",
    "print(sentiment_analysis(review_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f721a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_3 = \"\"\"\n",
    "I bought the PixelPioneer Quantum 70\" and it's simply fantastic.\n",
    "The voice control remote is a game-changer.\n",
    "However, the delivery was delayed by a week which was quite frustrating. - Emma, London\"\"\"\n",
    "\n",
    "print(sentiment_analysis(review_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4702ae9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c84477d",
   "metadata": {},
   "source": [
    "## Translation to other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f186a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, target_language, llm=llm_model):\n",
    "    global total_cost\n",
    "    \n",
    "    translation_template_string = \"\"\"\n",
    "    Translate the text delimited by tripple backticks into {language}.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    translation_prompt_template = PromptTemplate.from_template(translation_template_string)\n",
    "    \n",
    "    model_input = translation_prompt_template.format(text=text, language=target_language)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_text = \"Some of the capabilities of GPT-4 include describing humor in images, \\\n",
    "summarizing text from screenshots, and answering exam questions with diagrams.\"\n",
    "\n",
    "spanish_translation = translate(english_text, \"Spanish\")\n",
    "\n",
    "print(spanish_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "italian_translation = translate(english_text, \"Italian\")\n",
    "\n",
    "print(italian_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef104f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(translate(spanish_translation, \"Italian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quote from Wikipedia: https://el.wikipedia.org/wiki/GPT-4\n",
    "\n",
    "greek_text = \"Ως μετασχηματιστής, το GPT-4 ήταν προεκπαιδευμένο για την πρόβλεψη του επόμενου διακριτικού \\\n",
    "(χρησιμοποιώντας δημόσια δεδομένα και «δεδομένα με άδεια από τρίτους παρόχους») και στη συνέχεια βελτιστοποιήθηκε \\\n",
    "με ενισχυτική μάθηση από την ανάδραση ανθρώπου και τεχνητής νοημοσύνης για ανθρώπινη ευθυγράμμιση και πολιτική συμμόρφωση.\"\n",
    "\n",
    "print(translate(greek_text, \"English\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f2fb5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a1f9f8",
   "metadata": {},
   "source": [
    "## Rephrasing to correct or address a need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb85dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text, llm=llm_model):\n",
    "    global total_cost\n",
    "    \n",
    "    correct_grammar_template_string = \"\"\"\n",
    "    Correct grammar, punctuation and spelling in the text delimited by tripple backticks.\\\n",
    "    text: ```{text}```\n",
    "    \"\"\"\n",
    "    correct_grammar_prompt_template = PromptTemplate.from_template(correct_grammar_template_string)\n",
    "    \n",
    "    model_input = correct_grammar_prompt_template.format(text=text)\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = llm(model_input)\n",
    "        \n",
    "    total_cost += cb.total_cost\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"\n",
    "The model has limitations, including the tendency to hallucinate and lack transparency\n",
    "in its decision-making processes. It has also been found to have cognitive biases.\"\"\"\n",
    "\n",
    "altered_text = \"\"\"\n",
    "The mdel has limmitaions including, the tendency to halucinate and lsck trespacy\n",
    "in its decision making processes. It has also been fond to hav cognitive biasses.\"\"\"\n",
    "\n",
    "print(correct_text(altered_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47529fd5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dad0fda",
   "metadata": {},
   "source": [
    "## Examples of other ChatGPT based applications:\n",
    "\n",
    "- Chatbots\n",
    "- Question answering over documents\n",
    "- Customer support agents\n",
    "- Querying and analyzis of structured data\n",
    "- Personal assistants, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86a0fb4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee4680",
   "metadata": {},
   "source": [
    "# Get the total cost of running ChatGPT API calls in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bdd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total cost: ${total_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3914c79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
