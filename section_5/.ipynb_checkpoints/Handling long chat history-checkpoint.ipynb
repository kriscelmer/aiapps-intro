{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3ae59a1",
   "metadata": {},
   "source": [
    "# Handling long chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397607fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from IPython.display import display, HTML, Markdown\n",
    "from pprint import pprint\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aae70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import OpenAICallbackHandler\n",
    "\n",
    "totals_cb = OpenAICallbackHandler()\n",
    "\n",
    "print(totals_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc54f3",
   "metadata": {},
   "source": [
    "### `ConversationBufferWindowMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441437dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "conversation_window_chain = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0.0),\n",
    "    memory=ConversationBufferWindowMemory(k=2),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "get_response = lambda x: conversation_window_chain(x, callbacks=[totals_cb])['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6259bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Tell me briefly about the winner of the Battle of Thermopylae\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4de5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"When he ascended the throne?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5350bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"When he died?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b159ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Who was the next king?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e08515",
   "metadata": {},
   "source": [
    "### `ConversationTokenBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01234262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "conversation_token_chain = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0.0),\n",
    "    memory=ConversationTokenBufferMemory(\n",
    "        llm=ChatOpenAI(), # used to count number of tokens in chat history messages\n",
    "        max_token_limit=150\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "get_response = lambda x: conversation_token_chain(x, callbacks=[totals_cb])['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09746150",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Tell me briefly about Pandas dataframe\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"How can we compare it to a spreadsheet?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8913ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Is it possible to put values of different data types into one column of a dataframe?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8155b0",
   "metadata": {},
   "source": [
    "Check the source code of [`ConversationTokenBufferMemory`](https://api.python.langchain.com/en/latest/_modules/langchain/memory/token_buffer.html#ConversationTokenBufferMemory) to see how this is implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff099d",
   "metadata": {},
   "source": [
    "### `ConversationSummaryMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf7e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "conversation_summary_chain = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0.0),\n",
    "    memory=ConversationSummaryMemory(\n",
    "        llm=ChatOpenAI(temperature=0.0), # used to summarize messages in the chat history\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "get_response = lambda x: conversation_summary_chain(x, callbacks=[totals_cb])['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc18684",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_summary_chain.memory.prompt.template) # summarizer template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d532dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(conversation_summary_chain.memory.buffer) # summarizer buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6099b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Tell me briefly about numpy library\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd51d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_summary_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb86532",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Give me an example of use case\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Is it good for EDA with data sets like Titanic or better use other libraries?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb6561",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_summary_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8579a",
   "metadata": {},
   "source": [
    "### `ConversationSummaryBufferMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation_summary_buffer_chain = ConversationChain(\n",
    "    llm=ChatOpenAI(temperature=0.0),\n",
    "    memory=ConversationSummaryBufferMemory(\n",
    "        llm=ChatOpenAI(temperature=0.0), # used to summarize messages in the chat history\n",
    "        max_token_limit=500\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "get_response = lambda x: conversation_summary_buffer_chain(x, callbacks=[totals_cb])['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3f1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_summary_buffer_chain.prompt.template) # chain template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49abb3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation_summary_buffer_chain.memory.prompt.template) # summarizer template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f159d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Tell me about matplotlib library\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5481ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(conversation_summary_buffer_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0860a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"Give me a code example to plot a pie chart\")\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6951ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(\"How to plot the same data as a bar chart?\")\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dd7b1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5492fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "totals_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eade8e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
